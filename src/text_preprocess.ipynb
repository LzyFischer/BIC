{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "path1 = Path('../../../czl/TwiBot22-baselines/datasets/Twibot-20')\n",
    "path2 = Path('../../../czl/TwiBot22-baselines/data')\n",
    "path3 = Path('../../../czl/TwiBot22-baselines/src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(label):\n",
    "    if label[1] == 'human':\n",
    "        return (label[0], 1)\n",
    "    else:\n",
    "        return (label[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_info = pd.read_json(path1 / 'node.json')\n",
    "label = pd.read_csv(path1 / 'label.csv')\n",
    "split = pd.read_csv(path1 / 'split.csv')\n",
    "node_info = pd.merge(node_info, label)\n",
    "node_info = pd.merge(node_info, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_des = node_info['description']\n",
    "node_name = node_info['username']\n",
    "node_label = node_info['label']\n",
    "node_split = node_info['split']\n",
    "\n",
    "node_des = dict(map(lambda x: x, enumerate(node_des)))\n",
    "node_name = dict(map(lambda x: x, enumerate(node_name)))\n",
    "node_label = dict(map(str_to_bool, enumerate(node_label)))\n",
    "node_split = dict(map(lambda x: x, enumerate(node_split)))\n",
    "\n",
    "np.save('data/node_des.npy', node_des)\n",
    "np.save('data/node_name.npy', node_name)\n",
    "np.save('data/node_label.npy', node_label)\n",
    "np.save('data/node_split.npy', node_split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "below name->tensor\n",
    "\"\"\"\n",
    "from transformers import RobertaTokenizer, RobertaModel, pipeline\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "feature_extractor = pipeline('feature-extraction', model=RobertaModel.from_pretrained(model_name), tokenizer=tokenizer, device=0)\n",
    "\n",
    "node_name = np.load('data/node_name.npy', allow_pickle=True).tolist()\n",
    "\n",
    "names_vector = []\n",
    "for i in tqdm(range(len(node_name))):\n",
    "    name_vector = torch.tensor(feature_extractor(node_name[i])).squeeze(0)\n",
    "    name_vector = torch.mean(name_vector, dim=0)\n",
    "    names_vector.append(name_vector)\n",
    "    \n",
    "torch.stack(names_vector, dim=0)\n",
    "torch.save(names_vector, 'data/names_vector.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "note: len(users_tweets == 229580) , len(node_des == 11826)\n",
    "\"\"\"\n",
    "des_vector = torch.load(path2 / 'des_tensor.pt')\n",
    "names_vector = torch.load('data/names_vector.pt')\n",
    "users_tweets = np.load(path3 / 'T5/Twibot-20/data/user_tweets_dict.npy', allow_pickle=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11826/11826 [00:00<00:00, 19081.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "concat text\n",
    "\"\"\"\n",
    "users_text = {}\n",
    "for i in tqdm(range(len(node_des))):\n",
    "    user_tweets = ''.join(users_tweets[i])\n",
    "    users_text[i] = '. '.join([node_name[i].strip(' '), node_des[i].strip(' '), user_tweets])\n",
    "    \n",
    "np.save('data/users_text.npy', users_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2848342937cb880315c3e3d7c3f0c6227ac0bdefbde89a99aafde78196732fb8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('whr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
